{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208de9e5",
   "metadata": {},
   "source": [
    "# üè¶ Project: Loan Eligibility Prediction\n",
    "## üöÄ Phase 2: Data Preprocessing & Machine Learning Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ **Overview**\n",
    "Welcome to the engine room of the project. After exploring the data in **Phase 1 (EDA)**, we now transition to building predictive models. The goal is to automate the loan eligibility process (real-time) based on customer details provided while filling out online application forms.\n",
    "\n",
    "### üéØ **The Mission**\n",
    "To build a robust binary classifier that predicts `Loan_Status` (Approved/Rejected).\n",
    "* **Business Goal:** Minimize risk for the bank while ensuring eligible applicants aren't turned away.\n",
    "* **Key Metrics:** We prioritize **Accuracy** and **Weighted F1-Score** to balance precision and recall.\n",
    "\n",
    "### ‚öôÔ∏è **Notebook Workflow**\n",
    "1.  **Preprocessing & Imputation:** Using `KNNImputer` for numerical gaps and Mode for categorical gaps.\n",
    "2.  **Feature Engineering:** Log-transforming skewed financial data (`Total_Income`) and One-Hot Encoding categories.\n",
    "3.  **Baseline Screening:** Testing **14 different algorithms** (Linear, Trees, Ensembles, SVMs) to find top performers.\n",
    "4.  **Hyperparameter Tuning:** Using `GridSearchCV` to optimize the best candidates.\n",
    "5.  **Final Selection:** Choosing the \"Champion Model\" for the final evaluation phase.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a717f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Manipulation ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scikit-Learn: Preprocessing & Imputation ---\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer  # For Log scaling\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# --- Scikit-Learn: Models ---\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# --- Scikit-Learn: Metrics ---\n",
    "from sklearn.metrics import accuracy_score, f1_score, average_precision_score, confusion_matrix, classification_report, precision_score, recall_score\n",
    "\n",
    "# --- Configuration ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486686c",
   "metadata": {},
   "source": [
    "## 2. üìÇ Data Loading & Initial Inspection\n",
    "We load the preprocessed dataset saved from the previous EDA phase.\n",
    "* **Source:** `preprocessed_loan.csv`\n",
    "* **Action:** verification of the first few rows to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb225d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>Total_Income</th>\n",
       "      <th>Loan_Amount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4941.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   Total_Income  Loan_Amount  Loan_Amount_Term  Credit_History Property_Area  \\\n",
       "0        5849.0          NaN             360.0             1.0         Urban   \n",
       "1        6091.0        128.0             360.0             1.0         Rural   \n",
       "2        3000.0         66.0             360.0             1.0         Urban   \n",
       "3        4941.0        120.0             360.0             1.0         Urban   \n",
       "4        6000.0        141.0             360.0             1.0         Urban   \n",
       "\n",
       "  Loan_Status  \n",
       "0           Y  \n",
       "1           N  \n",
       "2           Y  \n",
       "3           Y  \n",
       "4           Y  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"..\\data\\preprocessed_loan.csv\")\n",
    "\n",
    "# Display the first 5 rows to check structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24156d",
   "metadata": {},
   "source": [
    "## 3. üõ†Ô∏è Feature Formatting & Target Encoding\n",
    "Before splitting the data, we must ensure features have the correct data types.\n",
    "\n",
    "1.  **Loan_Amount_Term:** Converted to `object` (categorical) because loan terms are discrete categories (e.g., 360 months, 180 months), not continuous numbers.\n",
    "2.  **Credit_History:** Converted to `category` as it represents a binary state (0 or 1).\n",
    "3.  **Loan_Status (Target):** We map the target variable `'Y'`/`'N'` to binary `1`/`0` for machine learning compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa5a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Class Distribution (Before Encoding):\n",
      "Loan_Status\n",
      "Y    422\n",
      "N    192\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   Loan_ID           614 non-null    object  \n",
      " 1   Gender            601 non-null    object  \n",
      " 2   Married           611 non-null    object  \n",
      " 3   Dependents        599 non-null    object  \n",
      " 4   Education         614 non-null    object  \n",
      " 5   Self_Employed     582 non-null    object  \n",
      " 6   Total_Income      614 non-null    float64 \n",
      " 7   Loan_Amount       592 non-null    float64 \n",
      " 8   Loan_Amount_Term  600 non-null    object  \n",
      " 9   Credit_History    564 non-null    category\n",
      " 10  Property_Area     614 non-null    object  \n",
      " 11  Loan_Status       614 non-null    int64   \n",
      "dtypes: category(1), float64(2), int64(1), object(8)\n",
      "memory usage: 53.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# 1. Cast Loan_Amount_Term to Int64 (handles NaNs) then to object (categorical)\n",
    "df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype('Int64')\n",
    "df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype('object')\n",
    "\n",
    "# 2. Cast Credit_History to category\n",
    "df['Credit_History'] = df['Credit_History'].astype('category')\n",
    "\n",
    "# 3. Check distribution of the target variable before encoding\n",
    "print(\"Target Class Distribution (Before Encoding):\")\n",
    "print(df['Loan_Status'].value_counts())\n",
    "\n",
    "# 4. Encode Target: Y -> 1 (Approved), N -> 0 (Rejected)\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y':1, 'N':0})\n",
    "\n",
    "# Verify the changes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b3d30",
   "metadata": {},
   "source": [
    "## 4. üìä Feature Engineering Strategy\n",
    "We separate our features into **Numerical** and **Categorical** groups. This is crucial because they require different preprocessing pipelines:\n",
    "* **Numeric:** Requires scaling (to handle outliers like high incomes).\n",
    "* **Categorical:** Requires encoding (to convert text labels to numbers).\n",
    "\n",
    "### **Feature Groups:**\n",
    "* **Target:** `Loan_Status`\n",
    "* **Numeric:** `Total_Income`, `Loan_Amount`\n",
    "* **Categorical:** Gender, Married, Dependents, Education, Self_Employed, Loan_Amount_Term, Credit_History, Property_Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46a781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Numeric Features: ['Total_Income', 'Loan_Amount']\n",
      "‚úÖ Categorical Features: ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "target_feature = 'Loan_Status'\n",
    "\n",
    "numeric_features = ['Total_Income', 'Loan_Amount']\n",
    "\n",
    "categorical_features = ['Gender',\n",
    "                        'Married',\n",
    "                        'Dependents',\n",
    "                        'Education',\n",
    "                        'Self_Employed',\n",
    "                        'Loan_Amount_Term',\n",
    "                        'Credit_History',\n",
    "                        'Property_Area'\n",
    "                            ]\n",
    "\n",
    "print(f\"‚úÖ Numeric Features: {numeric_features}\")\n",
    "print(f\"‚úÖ Categorical Features: {categorical_features}\")\n",
    "\n",
    "# Ensure all categorical features are strictly cast to type 'category'\n",
    "# This saves memory and ensures compatibility with certain sklearn selectors\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578f88d",
   "metadata": {},
   "source": [
    "## 5. ‚úÇÔ∏è Data Splitting (Train-Validation-Test)\n",
    "To build a robust model and prevent overfitting, we use a **three-way split strategy**:\n",
    "\n",
    "1.  **Training Set (64%):** Used to fit the models.\n",
    "2.  **Validation Set (16%):** Used for unbiased model evaluation and hyperparameter tuning during the development phase.\n",
    "3.  **Test Set (20%):** Held out completely until the very end to provide a final performance estimate.\n",
    "\n",
    "**Method:** We use `stratify=y` to ensure the proportion of Approved/Rejected loans remains consistent across all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0a652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape:   (392, 11)\n",
      "Validation Shape: (99, 11)\n",
      "Test Shape:       (123, 11)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop(\"Loan_Status\", axis=1)\n",
    "y = df[\"Loan_Status\"]\n",
    "\n",
    "# 1. First Split: Separate out the Test set (20%)\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Essential for imbalanced datasets\n",
    ")\n",
    "\n",
    "# 2. Second Split: Separate the remaining 80% into Train and Validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_temp, y_train_temp,\n",
    "    test_size=0.2, # 0.2 * 0.8 = 0.16 (16% of total data)\n",
    "    random_state=42,\n",
    "    stratify=y_train_temp\n",
    ")\n",
    "\n",
    "# Save sets to disk for reproducibility in other notebooks\n",
    "X_test.to_csv(r'../data/X_test.csv', index=False)\n",
    "y_test.to_csv(r'../data/y_test.csv', index=False)\n",
    "X_train.to_csv(r\"../data/X_train.csv\", index=False)\n",
    "y_train.to_csv(r\"../data/y_train.csv\", index=False)\n",
    "X_valid.to_csv(r\"../data/X_valid.csv\", index=False)\n",
    "y_valid.to_csv(r\"../data/y_valid.csv\", index=False)\n",
    "\n",
    "# Check the shape of the resulting splits\n",
    "print(f\"Training Shape:   {X_train.shape}\")\n",
    "print(f\"Validation Shape: {X_valid.shape}\")\n",
    "print(f\"Test Shape:       {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b845aa6",
   "metadata": {},
   "source": [
    "## 6. ‚öôÔ∏è Preprocessing Pipelines\n",
    "Machine learning models cannot handle missing values or raw text. We build **Pipelines** to automate the cleanup process. This ensures that the exact same transformations applied to the training set are applied to the test set, preventing **data leakage**.\n",
    "\n",
    "### **Transformation Strategy:**\n",
    "1.  **Numeric Pipeline (`num`):**\n",
    "    * **Imputation:** We use `KNNImputer` (K-Nearest Neighbors). Instead of just filling with the \"average,\" this looks at similar borrowers to guess the missing income or loan amount.\n",
    "    * **Scaling:** We use `np.log1p` (Log Transformation). Financial data (like Income) is often skewed. Log transformation makes it more \"normal\" (bell-curve shaped), which helps models like Logistic Regression and SVM.\n",
    "2.  **Categorical Pipeline (`cat`):**\n",
    "    * **Imputation:** We use `SimpleImputer(strategy='most_frequent')` to fill missing text with the most common category (Mode).\n",
    "    * **Encoding:** We use `OneHotEncoder` to convert categories (e.g., \"Graduate\", \"Not Graduate\") into binary columns (1s and 0s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe82a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define distinct steps for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('logtransformer', FunctionTransformer(np.log1p, validate=False)),  # Log transform for skewness\n",
    "    ('imputer', KNNImputer(n_neighbors=5))                   # Fill missing using neighbors\n",
    "])\n",
    "\n",
    "# 2. Define distinct steps for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),     # Fill missing with mode\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Convert to binary\n",
    "])\n",
    "\n",
    "# 3. Combine them into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "# Visualizing the Pipeline object\n",
    "print(\"‚úÖ Preprocessing pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930895a",
   "metadata": {},
   "source": [
    "## 7. üß™ Baseline Model Screening\n",
    "We don't know yet which algorithm will best understand the patterns in loan approvals. Therefore, we define a \"dictionary\" of distinct classifiers to test them all at once.\n",
    "\n",
    "**We are testing 14 different algorithms across 4 families:**\n",
    "1.  **Linear Models:** Logistic Regression, Ridge, SGD (Good baselines).\n",
    "2.  **Tree-Based:** Decision Tree, Random Forest, Extra Trees (Good for capturing non-linear complex rules).\n",
    "3.  **Boosting:** AdaBoost, Gradient Boosting (High performance, builds weak learners into strong ones).\n",
    "4.  **Others:** SVM, KNN, Naive Bayes (Gaussian/Bernoulli), Discriminant Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d0139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized 14 models (including class_weight='balanced' where supported).\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of models to evaluate (with class_weight='balanced' where supported)\n",
    "models = {\n",
    "    # Linear & Distance based\n",
    "    \"Logistic Regression\": LogisticRegression(class_weight='balanced'),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),        # does NOT support class_weight\n",
    "    \"SGD Classifier\": SGDClassifier(class_weight='balanced'),\n",
    "    \"KNN\": KNeighborsClassifier(),                # does NOT support class_weight\n",
    "    \"SVM\": SVC(class_weight='balanced'),\n",
    "\n",
    "    # Tree & Ensemble based\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight='balanced'),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(),        # does NOT support class_weight\n",
    "    \"AdaBoost\": AdaBoostClassifier(),             # does NOT support class_weight\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),  # does NOT support class_weight\n",
    "\n",
    "    # Bayesian & Discriminant\n",
    "    \"GaussianNB\": GaussianNB(),                   # does NOT support class_weight\n",
    "    \"BernoulliNB\": BernoulliNB(),                 # does NOT support class_weight\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),   # no class_weight\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis()  # no class_weight\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Initialized {len(models)} models (including class_weight='balanced' where supported).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7bc67",
   "metadata": {},
   "source": [
    "## 8. üèÉ‚Äç‚ôÇÔ∏è Model Training & Evaluation Loop\n",
    "Here, we iterate through our dictionary of 14 models. For each algorithm, we create a temporary **Pipeline** that:\n",
    "1.  **Accepts raw data.**\n",
    "2.  **Runs the Preprocessor** (imputes missing values, scales numbers, one-hot encodes text).\n",
    "3.  **Fits the Model** on the `X_train` data.\n",
    "4.  **Predicts** results on the `X_valid` (Validation) data.\n",
    "\n",
    "**Metrics Used:**\n",
    "* **Accuracy:** Overall correctness (Correct Predictions / Total Predictions).\n",
    "* **F1 Score (Weighted):** The harmonic mean of Precision and Recall. This is often a better metric than accuracy for loan datasets, where we want to balance the risk of approving bad loans vs. rejecting good ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33c29fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model training loop...\n",
      "   ‚úÖ Logistic Regression trained.\n",
      "   ‚úÖ Ridge Classifier trained.\n",
      "   ‚úÖ SGD Classifier trained.\n",
      "   ‚úÖ KNN trained.\n",
      "   ‚úÖ SVM trained.\n",
      "   ‚úÖ Decision Tree trained.\n",
      "   ‚úÖ Random Forest trained.\n",
      "   ‚úÖ Extra Trees trained.\n",
      "   ‚úÖ AdaBoost trained.\n",
      "   ‚úÖ Gradient Boosting trained.\n",
      "   ‚úÖ GaussianNB trained.\n",
      "   ‚úÖ BernoulliNB trained.\n",
      "   ‚úÖ Linear Discriminant Analysis trained.\n",
      "   ‚úÖ Quadratic Discriminant Analysis trained.\n",
      "üèÅ Loop finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yashd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"üöÄ Starting model training loop...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline for the specific model\n",
    "    # This ensures the preprocessor runs immediately before the model trains\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        (name, model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on Validation set\n",
    "    y_pred = final_pipeline.predict(X_valid)\n",
    "\n",
    "    # Store metrics\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_valid, y_pred),\n",
    "        \"F1 Score\": f1_score(y_valid, y_pred, average='weighted'),\n",
    "        \"Average Precision\": average_precision_score(y_valid, y_pred) \n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} trained.\")\n",
    "\n",
    "print(\"üèÅ Loop finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84db24b",
   "metadata": {},
   "source": [
    "## 9. üèÜ Performance Leaderboard\n",
    "We convert our results dictionary into a Pandas DataFrame to easily compare the models. We sort by **Accuracy** to see the top performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a96bab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Average Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.796131</td>\n",
       "      <td>0.795990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.782492</td>\n",
       "      <td>0.786745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.790994</td>\n",
       "      <td>0.797369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.782492</td>\n",
       "      <td>0.786745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.768464</td>\n",
       "      <td>0.777714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.772939</td>\n",
       "      <td>0.789336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.759596</td>\n",
       "      <td>0.773835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.767677</td>\n",
       "      <td>0.761606</td>\n",
       "      <td>0.792158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.699692</td>\n",
       "      <td>0.739964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.747475</td>\n",
       "      <td>0.737586</td>\n",
       "      <td>0.773458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy  F1 Score  Average Precision\n",
       "BernoulliNB                   0.818182  0.796131           0.795990\n",
       "Ridge Classifier              0.808081  0.782492           0.786745\n",
       "Gradient Boosting             0.808081  0.790994           0.797369\n",
       "Linear Discriminant Analysis  0.808081  0.782492           0.786745\n",
       "SVM                           0.797980  0.768464           0.777714\n",
       "Random Forest                 0.787879  0.772939           0.789336\n",
       "AdaBoost                      0.787879  0.759596           0.773835\n",
       "Logistic Regression           0.767677  0.761606           0.792158\n",
       "KNN                           0.747475  0.699692           0.739964\n",
       "Extra Trees                   0.747475  0.737586           0.773458"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to DataFrame and transpose so models are rows\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Sort by Accuracy to find the best models\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "# Display the top 10 models\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822eca4c",
   "metadata": {},
   "source": [
    "## 10. üéõÔ∏è Hyperparameter Tuning\n",
    "Based on the screening results, we select the top candidates for fine-tuning. We use **GridSearchCV** to exhaustively search through a specified parameter grid to find the optimal configuration for each model.\n",
    "\n",
    "**Selected Models for Tuning:**\n",
    "1.  **Bernoulli Naive Bayes:** performed surprisingly well; we will tune the smoothing parameter (`alpha`).\n",
    "2.  **Extra Trees:** A strong ensemble method; we will tune the number of trees and split criteria.\n",
    "3.  **Decision Tree:** Included as a simpler baseline to compare against the ensembles.\n",
    "4.  **Support Vector Machine (SVM):** A robust classifier; we will tune the regularization (`C`) and kernel type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4ff62ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated Parameter Grids for Best Models\n"
     ]
    }
   ],
   "source": [
    "# Parameter grid for top-performing models\n",
    "model_params = {\n",
    "    \n",
    "    # 1. Bernoulli Naive Bayes\n",
    "    \"BernoulliNB\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"model__alpha\": [0.01, 0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "            \"model__fit_prior\": [True, False],\n",
    "            \"model__binarize\": [0.0, 0.5, 1.0]  # added optional parameter\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 2. Ridge Classifier\n",
    "    \"Ridge Classifier\": {\n",
    "        \"model\": RidgeClassifier(),\n",
    "        \"params\": {\n",
    "            \"model__alpha\": [0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "            \"model__solver\": [\"auto\", \"svd\", \"lsqr\", \"sag\"]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # 3. Support Vector Machine (SVM)\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"model__C\": [0.1, 1, 10, 100],             # Regularization parameter\n",
    "            \"model__kernel\": [\"linear\", \"rbf\"],        # Kernel type\n",
    "            \"model__gamma\": [\"scale\", \"auto\"]          # Kernel coefficient\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 4. Gradient Boosting\n",
    "    \"Gradient Boosting\": {\n",
    "        \"model\": GradientBoostingClassifier(),\n",
    "        \"params\": {\n",
    "            \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "            \"model__n_estimators\": [100, 200, 300],\n",
    "            \"model__max_depth\": [2, 3, 5],\n",
    "            \"model__min_samples_split\": [2, 5],\n",
    "            \"model__min_samples_leaf\": [1, 2]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 5. Linear Discriminant Analysis\n",
    "    \"Linear Discriminant Analysis\": {\n",
    "        \"model\": LinearDiscriminantAnalysis(),\n",
    "        \"params\": {\n",
    "            # If using 'svd' ‚Üí shrinkage MUST be None\n",
    "            \"model__solver\": [\"svd\"],\n",
    "            \"model__shrinkage\": [None]\n",
    "            \n",
    "            # If using 'lsqr' or 'eigen' ‚Üí shrinkage MUST be provided\n",
    "            # \"model__solver\": [\"lsqr\", \"eigen\"],\n",
    "            # \"model__shrinkage\": [\"auto\", 0.1, 0.5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Updated Parameter Grids for Best Models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6047308",
   "metadata": {},
   "source": [
    "## 11. üîç Running GridSearchCV\n",
    "We loop through the selected models. For each combination of parameters, we use **5-Fold Cross-Validation**. This means the training data is split into 5 chunks; the model trains on 4 and tests on 1, rotating 5 times. This ensures the \"Best Score\" is reliable and not just a fluke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb7708ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Grid Search...\n",
      "   ‚úÖ BernoulliNB tuned. Best Score: 0.8375\n",
      "   ‚úÖ Ridge Classifier tuned. Best Score: 0.8393\n",
      "   ‚úÖ SVM tuned. Best Score: 0.8247\n",
      "   ‚úÖ Gradient Boosting tuned. Best Score: 0.8055\n",
      "   ‚úÖ Linear Discriminant Analysis tuned. Best Score: 0.8157\n",
      "üèÅ Tuning finished.\n"
     ]
    }
   ],
   "source": [
    "results_tuning = {}\n",
    "\n",
    "print(\"üöÄ Starting Grid Search...\")\n",
    "\n",
    "for name, mp in model_params.items():\n",
    "    # Construct the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', mp[\"model\"])\n",
    "    ])\n",
    "    \n",
    "    # Initialize Grid Search\n",
    "    # cv=5: 5-fold cross-validation\n",
    "    # scoring='accuracy': optimizing for accuracy\n",
    "    clf = GridSearchCV(pipe, mp[\"params\"], cv=5, scoring='average_precision', error_score=np.nan)\n",
    "    \n",
    "    # Fit on the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best results\n",
    "    results_tuning[name] = {\n",
    "        \"Best Parameters\": clf.best_params_,\n",
    "        \"Best Accuracy\": clf.best_score_\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} tuned. Best Score: {clf.best_score_:.4f}\")\n",
    "\n",
    "print(\"üèÅ Tuning finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba9113",
   "metadata": {},
   "source": [
    "## 12. üìä Tuning Results & Model Selection\n",
    "Let's view the best parameters found for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23dac1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Parameters</th>\n",
       "      <th>Best Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>{'model__alpha': 10.0, 'model__binarize': 0.0,...</td>\n",
       "      <td>0.837537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>{'model__alpha': 100.0, 'model__solver': 'auto'}</td>\n",
       "      <td>0.839289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>{'model__C': 1, 'model__gamma': 'auto', 'model...</td>\n",
       "      <td>0.824702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>{'model__learning_rate': 0.05, 'model__max_dep...</td>\n",
       "      <td>0.805467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>{'model__shrinkage': None, 'model__solver': 's...</td>\n",
       "      <td>0.815706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                Best Parameters  \\\n",
       "BernoulliNB                   {'model__alpha': 10.0, 'model__binarize': 0.0,...   \n",
       "Ridge Classifier               {'model__alpha': 100.0, 'model__solver': 'auto'}   \n",
       "SVM                           {'model__C': 1, 'model__gamma': 'auto', 'model...   \n",
       "Gradient Boosting             {'model__learning_rate': 0.05, 'model__max_dep...   \n",
       "Linear Discriminant Analysis  {'model__shrinkage': None, 'model__solver': 's...   \n",
       "\n",
       "                             Best Accuracy  \n",
       "BernoulliNB                       0.837537  \n",
       "Ridge Classifier                  0.839289  \n",
       "SVM                               0.824702  \n",
       "Gradient Boosting                 0.805467  \n",
       "Linear Discriminant Analysis      0.815706  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to DataFrame for easy viewing\n",
    "results_tuning_df = pd.DataFrame(results_tuning).T\n",
    "results_tuning_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9ab7d",
   "metadata": {},
   "source": [
    "## 13. üîÅ Final Evaluation ‚Äì Best Models After Hyperparameter Tuning\n",
    "\n",
    "Using the best parameters from GridSearchCV, we trained and evaluated all models again on the test set.\n",
    "\n",
    "For each model, we computed:\n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F1 Score  \n",
    "- Confusion Matrix  \n",
    "- Classification Report  \n",
    "\n",
    "A performance comparison table was generated to select the final model for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbf828a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Evaluating ALL Best Models on Test Set...\n",
      "\n",
      "üìå BernoulliNB - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.55      0.70        38\n",
      "           1       0.83      0.99      0.90        85\n",
      "\n",
      "    accuracy                           0.85       123\n",
      "   macro avg       0.89      0.77      0.80       123\n",
      "weighted avg       0.87      0.85      0.84       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21 17]\n",
      " [ 1 84]]\n",
      "------------------------------------------------------------\n",
      "üìå Ridge Classifier - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.55      0.70        38\n",
      "           1       0.83      0.99      0.90        85\n",
      "\n",
      "    accuracy                           0.85       123\n",
      "   macro avg       0.89      0.77      0.80       123\n",
      "weighted avg       0.87      0.85      0.84       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21 17]\n",
      " [ 1 84]]\n",
      "------------------------------------------------------------\n",
      "üìå SVM - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.55      0.70        38\n",
      "           1       0.83      0.99      0.90        85\n",
      "\n",
      "    accuracy                           0.85       123\n",
      "   macro avg       0.89      0.77      0.80       123\n",
      "weighted avg       0.87      0.85      0.84       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21 17]\n",
      " [ 1 84]]\n",
      "------------------------------------------------------------\n",
      "üìå Gradient Boosting - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.58      0.70        38\n",
      "           1       0.84      0.96      0.90        85\n",
      "\n",
      "    accuracy                           0.85       123\n",
      "   macro avg       0.86      0.77      0.80       123\n",
      "weighted avg       0.85      0.85      0.84       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22 16]\n",
      " [ 3 82]]\n",
      "------------------------------------------------------------\n",
      "üìå Linear Discriminant Analysis - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.55      0.70        38\n",
      "           1       0.83      0.99      0.90        85\n",
      "\n",
      "    accuracy                           0.85       123\n",
      "   macro avg       0.89      0.77      0.80       123\n",
      "weighted avg       0.87      0.85      0.84       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[21 17]\n",
      " [ 1 84]]\n",
      "------------------------------------------------------------\n",
      "üìå Random Forest - Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65        38\n",
      "           1       0.83      0.88      0.86        85\n",
      "\n",
      "    accuracy                           0.80       123\n",
      "   macro avg       0.77      0.74      0.75       123\n",
      "weighted avg       0.79      0.80      0.79       123\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23 15]\n",
      " [10 75]]\n",
      "------------------------------------------------------------\n",
      "\n",
      "üèÅ Evaluation Completed!\n"
     ]
    }
   ],
   "source": [
    "# Best parameters from GridSearchCV\n",
    "best_models = {\n",
    "    \"BernoulliNB\": BernoulliNB(alpha=10.0, binarize=0.0, fit_prior=True),\n",
    "\n",
    "    \"Ridge Classifier\": RidgeClassifier(alpha=100.0, solver=\"auto\"),\n",
    "\n",
    "    \"SVM\": SVC(C=1, gamma=\"auto\", kernel=\"rbf\", probability=True),   # probability=True ‚Üí For ROC Curve\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.05,\n",
    "                                                    max_depth=2,\n",
    "                                                    min_samples_leaf=1,\n",
    "                                                    min_samples_split=5,\n",
    "                                                    n_estimators=100,\n",
    "                                                    random_state=42\n",
    "                                                    ),\n",
    "\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(shrinkage=None,\n",
    "                                                               solver=\"svd\"\n",
    "                                                    ),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, \n",
    "                                            max_depth=None,\n",
    "                                            min_samples_leaf=2,\n",
    "                                            min_samples_split=5,\n",
    "                                            class_weight='balanced',\n",
    "                                            random_state=42\n",
    "                                                    )\n",
    "}\n",
    "\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "results_test = {}\n",
    "print(\"üöÄ Evaluating ALL Best Models on Test Set...\\n\")\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    # Create pipeline\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Train model\n",
    "    pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    # Store results\n",
    "    results_test[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "    # Print classification report & confusion matrix\n",
    "    print(f\"üìå {name} - Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nüèÅ Evaluation Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98b22799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.840441</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.988235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.840441</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.988235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.840441</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.988235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.840441</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.988235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.835078</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.964706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.792495</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy  F1 Score  Precision    Recall\n",
       "BernoulliNB                   0.853659  0.840441   0.831683  0.988235\n",
       "Ridge Classifier              0.853659  0.840441   0.831683  0.988235\n",
       "SVM                           0.853659  0.840441   0.831683  0.988235\n",
       "Linear Discriminant Analysis  0.853659  0.840441   0.831683  0.988235\n",
       "Gradient Boosting             0.845528  0.835078   0.836735  0.964706\n",
       "Random Forest                 0.796748  0.792495   0.833333  0.882353"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_test).T.sort_values(by=\"Accuracy\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df18c12",
   "metadata": {},
   "source": [
    "## 14. üèÅ Final Model Selection\n",
    "\n",
    "After evaluating the tuned models on the **test dataset**, four models demonstrated strong and consistent performance:\n",
    "- Support Vector Machine (SVM)\n",
    "- Bernoulli Naive Bayes (BernoulliNB)\n",
    "- Ridge Classifier\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "\n",
    "These models were shortlisted because:\n",
    "‚úî They achieved high F1-scores  \n",
    "‚úî They produced **only 1 False Negative** (critical for loan approval)  \n",
    "‚úî They showed excellent **recall for the approved class (Class 1)**  \n",
    "‚úî Suitable for business use ‚Äî minimal rejection of genuine loan applicants  \n",
    "\n",
    "---\n",
    "\n",
    "### üîç Business-Critical Evaluation Criteria\n",
    "\n",
    "| Critical Metric | Why It Matters in Loan Approval | Top Models |\n",
    "|-----------------|----------------------------------|-------------|\n",
    "| **False Negatives (FN)** | Rejecting valid applicants leads to business loss | SVM, BernoulliNB, Ridge, LDA |\n",
    "| **Recall (Class 1)** | Must approve almost all eligible applicants | SVM (Recall = 0.99) |\n",
    "| **F1-Score** | Balance between approval rate & risk | SVM & BernoulliNB |\n",
    "| **Interpretability** | Financial decisions require transparency | Ridge & BernoulliNB |\n",
    "| **Deployment Speed** | Needs real-time API usage | BernoulliNB |\n",
    "\n",
    "From a **domain (banking) perspective**, minimizing **False Negatives** is MOST important:\n",
    "> üö® Rejecting an eligible customer is worse than approving a slightly risky customer.\n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ Final Decision ‚Äî Selected Model\n",
    "\n",
    "We selected **Support Vector Machine (SVM)** as the **final model** for deployment because:\n",
    "- It achieved **only 1 False Negative**\n",
    "- Very high **recall (0.99)** for approved loans\n",
    "- Strong balance between precision and recall\n",
    "- Handles high-dimensional one-hot encoded data effectively\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öô Tuned Hyperparameters (From GridSearchCV)\n",
    "\n",
    "```python\n",
    "model = SVC(\n",
    "    C=1,\n",
    "    kernel='rbf',\n",
    "    gamma='auto',\n",
    "    probability=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7962458",
   "metadata": {},
   "source": [
    "### **Next Steps:**\n",
    "In the final evaluation notebook (`3. Model_Training_Test.ipynb`), we will:\n",
    "1.  Instantiate the SVM with these specific parameters.\n",
    "2.  Train it on the full training data.\n",
    "3.  Evaluate it on the unseen **Test Set** (X_test) to get the final performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

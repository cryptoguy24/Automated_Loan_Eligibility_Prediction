{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "208de9e5",
   "metadata": {},
   "source": [
    "# üè¶ Project: Loan Eligibility Prediction\n",
    "## üöÄ Phase 2: Data Preprocessing & Machine Learning Pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### üìñ **Overview**\n",
    "Welcome to the engine room of the project. After exploring the data in **Phase 1 (EDA)**, we now transition to building predictive models. The goal is to automate the loan eligibility process (real-time) based on customer details provided while filling out online application forms.\n",
    "\n",
    "### üéØ **The Mission**\n",
    "To build a robust binary classifier that predicts `Loan_Status` (Approved/Rejected).\n",
    "* **Business Goal:** Minimize risk for the bank while ensuring eligible applicants aren't turned away.\n",
    "* **Key Metrics:** We prioritize **Accuracy** and **Weighted F1-Score** to balance precision and recall.\n",
    "\n",
    "### ‚öôÔ∏è **Notebook Workflow**\n",
    "1.  **Preprocessing & Imputation:** Using `KNNImputer` for numerical gaps and Mode for categorical gaps.\n",
    "2.  **Feature Engineering:** Log-transforming skewed financial data (`Total_Income`) and One-Hot Encoding categories.\n",
    "3.  **Baseline Screening:** Testing **14 different algorithms** (Linear, Trees, Ensembles, SVMs) to find top performers.\n",
    "4.  **Hyperparameter Tuning:** Using `GridSearchCV` to optimize the best candidates.\n",
    "5.  **Final Selection:** Choosing the \"Champion Model\" for the final evaluation phase.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a717f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Manipulation ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Scikit-Learn: Preprocessing & Imputation ---\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer  # For Log scaling\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# --- Scikit-Learn: Models ---\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# --- Scikit-Learn: Metrics ---\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# --- Configuration ---\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4486686c",
   "metadata": {},
   "source": [
    "## 2. üìÇ Data Loading & Initial Inspection\n",
    "We load the preprocessed dataset saved from the previous EDA phase.\n",
    "* **Source:** `preprocessed_loan.csv`\n",
    "* **Action:** verification of the first few rows to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb225d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>Total_Income</th>\n",
       "      <th>Loan_Amount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6091.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4941.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   Total_Income  Loan_Amount  Loan_Amount_Term  Credit_History Property_Area  \\\n",
       "0        5849.0          NaN             360.0             1.0         Urban   \n",
       "1        6091.0        128.0             360.0             1.0         Rural   \n",
       "2        3000.0         66.0             360.0             1.0         Urban   \n",
       "3        4941.0        120.0             360.0             1.0         Urban   \n",
       "4        6000.0        141.0             360.0             1.0         Urban   \n",
       "\n",
       "  Loan_Status  \n",
       "0           Y  \n",
       "1           N  \n",
       "2           Y  \n",
       "3           Y  \n",
       "4           Y  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(r\"..\\data\\preprocessed_loan.csv\")\n",
    "\n",
    "# Display the first 5 rows to check structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24156d",
   "metadata": {},
   "source": [
    "## 3. üõ†Ô∏è Feature Formatting & Target Encoding\n",
    "Before splitting the data, we must ensure features have the correct data types.\n",
    "\n",
    "1.  **Loan_Amount_Term:** Converted to `object` (categorical) because loan terms are discrete categories (e.g., 360 months, 180 months), not continuous numbers.\n",
    "2.  **Credit_History:** Converted to `category` as it represents a binary state (0 or 1).\n",
    "3.  **Loan_Status (Target):** We map the target variable `'Y'`/`'N'` to binary `1`/`0` for machine learning compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa5a2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Class Distribution (Before Encoding):\n",
      "Loan_Status\n",
      "Y    422\n",
      "N    192\n",
      "Name: count, dtype: int64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 614 entries, 0 to 613\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype   \n",
      "---  ------            --------------  -----   \n",
      " 0   Loan_ID           614 non-null    object  \n",
      " 1   Gender            601 non-null    object  \n",
      " 2   Married           611 non-null    object  \n",
      " 3   Dependents        599 non-null    object  \n",
      " 4   Education         614 non-null    object  \n",
      " 5   Self_Employed     582 non-null    object  \n",
      " 6   Total_Income      614 non-null    float64 \n",
      " 7   Loan_Amount       592 non-null    float64 \n",
      " 8   Loan_Amount_Term  600 non-null    object  \n",
      " 9   Credit_History    564 non-null    category\n",
      " 10  Property_Area     614 non-null    object  \n",
      " 11  Loan_Status       614 non-null    int64   \n",
      "dtypes: category(1), float64(2), int64(1), object(8)\n",
      "memory usage: 53.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# 1. Cast Loan_Amount_Term to Int64 (handles NaNs) then to object (categorical)\n",
    "df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype('Int64')\n",
    "df['Loan_Amount_Term'] = df['Loan_Amount_Term'].astype('object')\n",
    "\n",
    "# 2. Cast Credit_History to category\n",
    "df['Credit_History'] = df['Credit_History'].astype('category')\n",
    "\n",
    "# 3. Check distribution of the target variable before encoding\n",
    "print(\"Target Class Distribution (Before Encoding):\")\n",
    "print(df['Loan_Status'].value_counts())\n",
    "\n",
    "# 4. Encode Target: Y -> 1 (Approved), N -> 0 (Rejected)\n",
    "df['Loan_Status'] = df['Loan_Status'].map({'Y':1, 'N':0})\n",
    "\n",
    "# Verify the changes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b3d30",
   "metadata": {},
   "source": [
    "## 4. üìä Feature Engineering Strategy\n",
    "We separate our features into **Numerical** and **Categorical** groups. This is crucial because they require different preprocessing pipelines:\n",
    "* **Numeric:** Requires scaling (to handle outliers like high incomes).\n",
    "* **Categorical:** Requires encoding (to convert text labels to numbers).\n",
    "\n",
    "### **Feature Groups:**\n",
    "* **Target:** `Loan_Status`\n",
    "* **Numeric:** `Total_Income`, `Loan_Amount`\n",
    "* **Categorical:** Gender, Married, Dependents, Education, Self_Employed, Loan_Amount_Term, Credit_History, Property_Area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46a781c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Numeric Features: ['Total_Income', 'Loan_Amount']\n",
      "‚úÖ Categorical Features: ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Property_Area']\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups\n",
    "target_feature = 'Loan_Status'\n",
    "\n",
    "numeric_features = ['Total_Income', 'Loan_Amount']\n",
    "\n",
    "categorical_features = ['Gender',\n",
    "                        'Married',\n",
    "                        'Dependents',\n",
    "                        'Education',\n",
    "                        'Self_Employed',\n",
    "                        'Loan_Amount_Term',\n",
    "                        'Credit_History',\n",
    "                        'Property_Area'\n",
    "                            ]\n",
    "\n",
    "print(f\"‚úÖ Numeric Features: {numeric_features}\")\n",
    "print(f\"‚úÖ Categorical Features: {categorical_features}\")\n",
    "\n",
    "# Ensure all categorical features are strictly cast to type 'category'\n",
    "# This saves memory and ensures compatibility with certain sklearn selectors\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578f88d",
   "metadata": {},
   "source": [
    "## 5. ‚úÇÔ∏è Data Splitting (Train-Validation-Test)\n",
    "To build a robust model and prevent overfitting, we use a **three-way split strategy**:\n",
    "\n",
    "1.  **Training Set (64%):** Used to fit the models.\n",
    "2.  **Validation Set (16%):** Used for unbiased model evaluation and hyperparameter tuning during the development phase.\n",
    "3.  **Test Set (20%):** Held out completely until the very end to provide a final performance estimate.\n",
    "\n",
    "**Method:** We use `stratify=y` to ensure the proportion of Approved/Rejected loans remains consistent across all three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0a652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape:   (392, 11)\n",
      "Validation Shape: (99, 11)\n",
      "Test Shape:       (123, 11)\n"
     ]
    }
   ],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop(\"Loan_Status\", axis=1)\n",
    "y = df[\"Loan_Status\"]\n",
    "\n",
    "# 1. First Split: Separate out the Test set (20%)\n",
    "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y  # Essential for imbalanced datasets\n",
    ")\n",
    "\n",
    "# 2. Second Split: Separate the remaining 80% into Train and Validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_temp, y_train_temp,\n",
    "    test_size=0.2, # 0.2 * 0.8 = 0.16 (16% of total data)\n",
    "    random_state=42,\n",
    "    stratify=y_train_temp\n",
    ")\n",
    "\n",
    "# Optional: Save sets to disk for reproducibility in other notebooks\n",
    "# X_test.to_csv(r'../data/X_test.csv', index=False)\n",
    "# y_test.to_csv(r'../data/y_test.csv', index=False)\n",
    "# X_train.to_csv(r\"../data/X_train.csv\", index=False)\n",
    "# y_train.to_csv(r\"../data/y_train.csv\", index=False)\n",
    "# X_valid.to_csv(r\"../data/X_valid.csv\", index=False)\n",
    "# y_valid.to_csv(r\"../data/y_valid.csv\", index=False)\n",
    "\n",
    "# Check the shape of the resulting splits\n",
    "print(f\"Training Shape:   {X_train.shape}\")\n",
    "print(f\"Validation Shape: {X_valid.shape}\")\n",
    "print(f\"Test Shape:       {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b845aa6",
   "metadata": {},
   "source": [
    "## 6. ‚öôÔ∏è Preprocessing Pipelines\n",
    "Machine learning models cannot handle missing values or raw text. We build **Pipelines** to automate the cleanup process. This ensures that the exact same transformations applied to the training set are applied to the test set, preventing **data leakage**.\n",
    "\n",
    "### **Transformation Strategy:**\n",
    "1.  **Numeric Pipeline (`num`):**\n",
    "    * **Imputation:** We use `KNNImputer` (K-Nearest Neighbors). Instead of just filling with the \"average,\" this looks at similar borrowers to guess the missing income or loan amount.\n",
    "    * **Scaling:** We use `np.log1p` (Log Transformation). Financial data (like Income) is often skewed. Log transformation makes it more \"normal\" (bell-curve shaped), which helps models like Logistic Regression and SVM.\n",
    "2.  **Categorical Pipeline (`cat`):**\n",
    "    * **Imputation:** We use `SimpleImputer(strategy='most_frequent')` to fill missing text with the most common category (Mode).\n",
    "    * **Encoding:** We use `OneHotEncoder` to convert categories (e.g., \"Graduate\", \"Not Graduate\") into binary columns (1s and 0s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe82a23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Preprocessing pipeline created successfully.\n"
     ]
    }
   ],
   "source": [
    "# 1. Define distinct steps for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),                   # Fill missing using neighbors\n",
    "    ('logtransformer', FunctionTransformer(np.log1p, validate=False)) # Log transform for skewness\n",
    "])\n",
    "\n",
    "# 2. Define distinct steps for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),     # Fill missing with mode\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)) # Convert to binary\n",
    "])\n",
    "\n",
    "# 3. Combine them into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ])\n",
    "\n",
    "# Visualizing the Pipeline object\n",
    "print(\"‚úÖ Preprocessing pipeline created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930895a",
   "metadata": {},
   "source": [
    "## 7. üß™ Baseline Model Screening\n",
    "We don't know yet which algorithm will best understand the patterns in loan approvals. Therefore, we define a \"dictionary\" of distinct classifiers to test them all at once.\n",
    "\n",
    "**We are testing 14 different algorithms across 4 families:**\n",
    "1.  **Linear Models:** Logistic Regression, Ridge, SGD (Good baselines).\n",
    "2.  **Tree-Based:** Decision Tree, Random Forest, Extra Trees (Good for capturing non-linear complex rules).\n",
    "3.  **Boosting:** AdaBoost, Gradient Boosting (High performance, builds weak learners into strong ones).\n",
    "4.  **Others:** SVM, KNN, Naive Bayes (Gaussian/Bernoulli), Discriminant Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d0139e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Initialized 14 models for screening.\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of models to evaluate\n",
    "models = {\n",
    "    # Linear & Distance based\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"SGD Classifier\": SGDClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "\n",
    "    # Tree & Ensemble based\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "\n",
    "    # Bayesian & Discriminant\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"Linear Discriminant Analysis\": LinearDiscriminantAnalysis(),\n",
    "    \"Quadratic Discriminant Analysis\": QuadraticDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Initialized {len(models)} models for screening.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7bc67",
   "metadata": {},
   "source": [
    "## 8. üèÉ‚Äç‚ôÇÔ∏è Model Training & Evaluation Loop\n",
    "Here, we iterate through our dictionary of 14 models. For each algorithm, we create a temporary **Pipeline** that:\n",
    "1.  **Accepts raw data.**\n",
    "2.  **Runs the Preprocessor** (imputes missing values, scales numbers, one-hot encodes text).\n",
    "3.  **Fits the Model** on the `X_train` data.\n",
    "4.  **Predicts** results on the `X_valid` (Validation) data.\n",
    "\n",
    "**Metrics Used:**\n",
    "* **Accuracy:** Overall correctness (Correct Predictions / Total Predictions).\n",
    "* **F1 Score (Weighted):** The harmonic mean of Precision and Recall. This is often a better metric than accuracy for loan datasets, where we want to balance the risk of approving bad loans vs. rejecting good ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c29fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model training loop...\n",
      "   ‚úÖ Logistic Regression trained.\n",
      "   ‚úÖ Ridge Classifier trained.\n",
      "   ‚úÖ SGD Classifier trained.\n",
      "   ‚úÖ KNN trained.\n",
      "   ‚úÖ SVM trained.\n",
      "   ‚úÖ Decision Tree trained.\n",
      "   ‚úÖ Random Forest trained.\n",
      "   ‚úÖ Extra Trees trained.\n",
      "   ‚úÖ AdaBoost trained.\n",
      "   ‚úÖ Gradient Boosting trained.\n",
      "   ‚úÖ GaussianNB trained.\n",
      "   ‚úÖ BernoulliNB trained.\n",
      "   ‚úÖ Linear Discriminant Analysis trained.\n",
      "   ‚úÖ Quadratic Discriminant Analysis trained.\n",
      "üèÅ Loop finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yashd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n",
      "c:\\Users\\yashd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "print(\"üöÄ Starting model training loop...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline for the specific model\n",
    "    # This ensures the preprocessor runs immediately before the model trains\n",
    "    final_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        (name, model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    final_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on Validation set\n",
    "    y_pred = final_pipeline.predict(X_valid)\n",
    "\n",
    "    # Store metrics\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_valid, y_pred),\n",
    "        \"F1 Score\": f1_score(y_valid, y_pred, average='weighted') \n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} trained.\")\n",
    "\n",
    "print(\"üèÅ Loop finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84db24b",
   "metadata": {},
   "source": [
    "## 9. üèÜ Performance Leaderboard\n",
    "We convert our results dictionary into a Pandas DataFrame to easily compare the models. We sort by **Accuracy** to see the top performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a96bab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.796131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge Classifier</th>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.782492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Discriminant Analysis</th>\n",
       "      <td>0.808081</td>\n",
       "      <td>0.782492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.768464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGD Classifier</th>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.768464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.768464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.785473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.759596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.768994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Extra Trees</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.770568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Accuracy  F1 Score\n",
       "BernoulliNB                   0.818182  0.796131\n",
       "Ridge Classifier              0.808081  0.782492\n",
       "Linear Discriminant Analysis  0.808081  0.782492\n",
       "SVM                           0.797980  0.768464\n",
       "SGD Classifier                0.797980  0.768464\n",
       "Logistic Regression           0.797980  0.768464\n",
       "Random Forest                 0.797980  0.785473\n",
       "AdaBoost                      0.787879  0.759596\n",
       "Gradient Boosting             0.787879  0.768994\n",
       "Extra Trees                   0.777778  0.770568"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert results to DataFrame and transpose so models are rows\n",
    "results_df = pd.DataFrame(results).T\n",
    "\n",
    "# Sort by Accuracy to find the best models\n",
    "results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
    "\n",
    "# Display the top 10 models\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822eca4c",
   "metadata": {},
   "source": [
    "## 10. üéõÔ∏è Hyperparameter Tuning\n",
    "Based on the screening results, we select the top candidates for fine-tuning. We use **GridSearchCV** to exhaustively search through a specified parameter grid to find the optimal configuration for each model.\n",
    "\n",
    "**Selected Models for Tuning:**\n",
    "1.  **Bernoulli Naive Bayes:** performed surprisingly well; we will tune the smoothing parameter (`alpha`).\n",
    "2.  **Extra Trees:** A strong ensemble method; we will tune the number of trees and split criteria.\n",
    "3.  **Decision Tree:** Included as a simpler baseline to compare against the ensembles.\n",
    "4.  **Support Vector Machine (SVM):** A robust classifier; we will tune the regularization (`C`) and kernel type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c7845c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parameter grids defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for selected models\n",
    "model_params = {\n",
    "    # 1. Bernoulli Naive Bayes\n",
    "    \"BernoulliNB\": {\n",
    "        \"model\": BernoulliNB(),\n",
    "        \"params\": {\n",
    "            \"model__alpha\": [0.01, 0.1, 0.5, 1.0, 5.0, 10.0], # Smoothing parameter\n",
    "            \"model__fit_prior\": [True, False]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 2. Extra Trees Classifier\n",
    "    \"Extra Trees\": {\n",
    "        \"model\": ExtraTreesClassifier(),\n",
    "        \"params\": {\n",
    "            \"model__n_estimators\": [50, 100, 200],        # Number of trees\n",
    "            \"model__max_depth\": [None, 5, 10, 20],        # Max depth of tree\n",
    "            \"model__min_samples_split\": [2, 5, 10],       # Min samples required to split\n",
    "            \"model__min_samples_leaf\": [1, 2, 4],         # Min samples in a leaf\n",
    "            \"model__bootstrap\": [True, False]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 3. Decision Tree\n",
    "    \"Decision Tree\": {\n",
    "        \"model\": DecisionTreeClassifier(),\n",
    "        \"params\": {\n",
    "            \"model__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "            \"model__max_depth\": [None, 5, 10, 20],\n",
    "            \"model__min_samples_split\": [2, 5, 10],\n",
    "            \"model__min_samples_leaf\": [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # 4. Support Vector Machine (SVM)\n",
    "    \"SVM\": {\n",
    "        \"model\": SVC(),\n",
    "        \"params\": {\n",
    "            \"model__C\": [0.1, 1, 10, 100],             # Regularization parameter\n",
    "            \"model__kernel\": [\"linear\", \"rbf\"],        # Kernel type\n",
    "            \"model__gamma\": [\"scale\", \"auto\"]          # Kernel coefficient\n",
    "        }\n",
    "    }\n",
    "}\n",
    "print(\"‚úÖ Parameter grids defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6047308",
   "metadata": {},
   "source": [
    "## 11. üîç Running GridSearchCV\n",
    "We loop through the selected models. For each combination of parameters, we use **5-Fold Cross-Validation**. This means the training data is split into 5 chunks; the model trains on 4 and tests on 1, rotating 5 times. This ensures the \"Best Score\" is reliable and not just a fluke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7708ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Grid Search...\n",
      "   ‚úÖ BernoulliNB tuned. Best Score: 0.7985\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "results_tuning = {}\n",
    "\n",
    "print(\"üöÄ Starting Grid Search...\")\n",
    "\n",
    "for name, mp in model_params.items():\n",
    "    # Construct the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', mp[\"model\"])\n",
    "    ])\n",
    "    \n",
    "    # Initialize Grid Search\n",
    "    # cv=5: 5-fold cross-validation\n",
    "    # scoring='accuracy': optimizing for accuracy\n",
    "    clf = GridSearchCV(pipe, mp[\"params\"], cv=5, scoring='accuracy', error_score=np.nan)\n",
    "    \n",
    "    # Fit on the training set\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Store the best results\n",
    "    results_tuning[name] = {\n",
    "        \"Best Parameters\": clf.best_params_,\n",
    "        \"Best Accuracy\": clf.best_score_\n",
    "    }\n",
    "    \n",
    "    print(f\"   ‚úÖ {name} tuned. Best Score: {clf.best_score_:.4f}\")\n",
    "\n",
    "print(\"üèÅ Tuning finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba9113",
   "metadata": {},
   "source": [
    "## 12. üìä Tuning Results & Model Selection\n",
    "Let's view the best parameters found for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dac1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy viewing\n",
    "results_tuning_df = pd.DataFrame(results_tuning).T\n",
    "results_tuning_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ae77af",
   "metadata": {},
   "source": [
    "## 13. ‚úÖ Final Model Selection\n",
    "**Conclusion:**\n",
    "After reviewing the Grid Search results, we observe strong performances from BernoulliNB, Extra Trees, and SVM.\n",
    "\n",
    "**Decision:** We will proceed with the **Support Vector Machine (SVM)** using the optimal hyperparameters identified during tuning.\n",
    "\n",
    "**Winning Hyperparameters:**\n",
    "* **C (Regularization):** `0.1` (Strong regularization, prevents overfitting)\n",
    "* **Kernel:** `linear` (The data is linearly separable in the high-dimensional space created by One-Hot Encoding)\n",
    "* **Gamma:** `scale`\n",
    "\n",
    "**Next Steps:**\n",
    "In the final evaluation notebook (`03_final_evaluation.ipynb`), we will:\n",
    "1.  Instantiate the SVM with these specific parameters.\n",
    "2.  Train it on the full training data.\n",
    "3.  Evaluate it on the unseen **Test Set** (X_test) to get the final performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
